{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1748971783603,
     "user": {
      "displayName": "Juan Pablo Perez Vargas",
      "userId": "11361655634266533980"
     },
     "user_tz": 300
    },
    "id": "0XyIyd0J2Pa9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanpabloperezvargas/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-20 11:34:22.978076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/juanpabloperezvargas/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Colab: asegurar versiones recientes\n",
    "# !pip install -q scikit-posthocs==0.7 seaborn==0.13\n",
    "# !pip install --upgrade \"tensorflow==2.16.*\" \"keras==3.*\"\n",
    "# !pip install scikit-learn\n",
    "\n",
    "from pathlib import Path\n",
    "import json, math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from tensorflow import keras\n",
    "RESULTS_ROOT = Path().resolve() / \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 229 modelos Keras\n"
     ]
    }
   ],
   "source": [
    "def discover_keras(root: Path):\n",
    "    \"\"\"\n",
    "    Devuelve rutas a todos los modelos Keras encontrados en el directorio\n",
    "    especificado. Se espera que los modelos estén en subdirectorios con\n",
    "    ARQ_*/ESC_*/rep_*/checkpoints/.\n",
    "    \"\"\"\n",
    "    return sorted(root.glob(\"ARQ_*/ESC_*/rep_*/checkpoints/*.keras\"))\n",
    "\n",
    "keras_paths = discover_keras(RESULTS_ROOT)\n",
    "print(f\"Encontrados {len(keras_paths)} modelos Keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module.py\n",
    "import numpy as np\n",
    "from typing import Literal, Optional\n",
    "import os, h5py, numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "import yaml                          # Lectura y mezcla de archivos YAML\n",
    "import sys               # Conversión IPython Notebook → .py\n",
    "from pathlib import Path             # Manejo robusto de rutas\n",
    "from importlib import import_module  # Import dinámico del modelo\n",
    "\n",
    "\n",
    "# Rutas base (ajusta a tu estructura en Google Drive si cambia)\n",
    "CONFIG_ROOT = Path().resolve() / \"configs\"\n",
    "MODELS_ROOT = Path().resolve() / \"models\"\n",
    "\n",
    "def load_config(exp_name:str):\n",
    "    exp_path = CONFIG_ROOT / \"experiments\" / f\"{exp_name}.yaml\"\n",
    "    exp_cfg  = yaml.safe_load(exp_path.read_text())\n",
    "\n",
    "    if \"_base_\" in exp_cfg:                                # herencia opcional\n",
    "        base_cfg = yaml.safe_load((CONFIG_ROOT / exp_cfg[\"_base_\"]).read_text())\n",
    "        cfg = {**base_cfg, **exp_cfg}                      # exp > default\n",
    "    else:\n",
    "        cfg = exp_cfg\n",
    "    return cfg\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────\n",
    "class SingleHDF5:\n",
    "    \"\"\"\n",
    "    Envuelve *un* .hdf5 proveniente de Kaggle.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    kaggle_dataset_id : str\n",
    "        slug «user/dataset» (ej. \"ilikepizzaanddrones/modulated-iq-signals\")\n",
    "    local_download_dir : str | Path\n",
    "        carpeta donde se guardará (y se buscará) el .hdf5\n",
    "    keys : dict | None\n",
    "        nombres de los grupos dentro del HDF5 (default {\"X\",\"Y\",\"Z\"})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        local_dir: str,\n",
    "        keys: dict,\n",
    "    ) -> None:\n",
    "\n",
    "        # 0) Descarga / búsqueda local\n",
    "        file_path = local_dir\n",
    "\n",
    "        # 1) Lectura a memoria\n",
    "        self.keys = keys or {\"X\": \"X\", \"Y\": \"Y\", \"Z\": \"Z\"}\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            self.X = f[self.keys[\"X\"]][:]\n",
    "            self.Y = f[self.keys[\"Y\"]][:]\n",
    "            self.Z = f[self.keys[\"Z\"]][:] if self.keys[\"Z\"] in f else None\n",
    "\n",
    "            if \"Effects\" in f:\n",
    "                grp   = f[\"Effects\"]\n",
    "                dtype = [(n, grp[n].dtype) for n in grp]\n",
    "                eff   = np.empty(len(self.X), dtype=dtype)\n",
    "                for n in grp: eff[n] = grp[n][:]\n",
    "                self.Effects = eff\n",
    "            else:\n",
    "                self.Effects = None\n",
    "\n",
    "        # índices activos (se sobre-escriben desde DataModule)\n",
    "        n = len(self.X)\n",
    "        self.train_idx = np.arange(n, dtype=np.int64)\n",
    "        self.val_idx   = np.empty(0, dtype=np.int64)\n",
    "\n",
    "\n",
    "    # ── API mínima (igual que antes) ─────────────────────────────\n",
    "    def register_indices(self, train_idx, val_idx):\n",
    "        self.train_idx = np.asarray(train_idx, dtype=np.int64)\n",
    "        self.val_idx   = np.asarray(val_idx,   dtype=np.int64)\n",
    "\n",
    "    def get_arrays(self, split: str = None):\n",
    "        if split is None: return self.X, self.Y\n",
    "        split = split.lower()\n",
    "        if split == \"train\": return self.X[self.train_idx], self.Y[self.train_idx]\n",
    "        if split == \"val\":   return self.X[self.val_idx],   self.Y[self.val_idx]\n",
    "        raise ValueError(\"split debe ser 'train' o 'val'\")\n",
    "\n",
    "    # ————————————————————————————————————————————————————————\n",
    "    def get_effects(\n",
    "        self,\n",
    "        *,\n",
    "        split: str = None,\n",
    "        fields: list[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Devuelve un structured-array con los efectos alineados al `split`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        split : \"train\" | \"val\" | None\n",
    "            None ⇒ dataset completo (o testset completo si proviene del DataModule).\n",
    "        fields : list[str] | None\n",
    "            Sub-conjunto de columnas a devolver. None ⇒ todas.\n",
    "        \"\"\"\n",
    "        if self.Effects is None:\n",
    "            raise ValueError(\"Este HDF5 no contiene grupo 'Effects'.\")\n",
    "\n",
    "        # Selección de índices según split\n",
    "        if split is None:\n",
    "            idx = (\n",
    "                np.arange(len(self.X))               # testset completo\n",
    "                if (not hasattr(self, \"train_idx\"))   # por seguridad\n",
    "                else self.train_idx                   # SingleHDF5 sin register\n",
    "            )\n",
    "        else:\n",
    "            split = split.lower()\n",
    "            if split == \"train\":\n",
    "                idx = self.train_idx\n",
    "            elif split == \"val\":\n",
    "                idx = self.val_idx\n",
    "            else:\n",
    "                raise ValueError(\"split debe ser 'train', 'val' o None\")\n",
    "\n",
    "        eff = self.Effects[idx]              # vista alineada\n",
    "        if fields is not None:\n",
    "            eff = eff[fields].copy()\n",
    "        return eff\n",
    "    \n",
    "    # ────────────────────────────────────────────────                    \n",
    "    def to_tf_dataset(\n",
    "        self,\n",
    "        *,                                      \n",
    "        split: str = None,\n",
    "        batch_size: int,\n",
    "        shuffle: bool = True,\n",
    "        seed: int,\n",
    "        prefetch: bool = True,\n",
    "        include_index: bool = False,\n",
    "        buffer_size: int = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Devuelve un tf.data.Dataset con (X, Y) o (X, Y, idx).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        split : \"train\" | \"val\" | None\n",
    "            None ⇒ dataset completo (sin barajar).\n",
    "        include_index : bool\n",
    "            Si True, añade el índice absoluto dentro del HDF5\n",
    "            (útil para métricas por muestra).\n",
    "        buffer_size : int | None\n",
    "            Tamaño del «shuffle buffer». Por defecto = len(split).\n",
    "        \"\"\"\n",
    "\n",
    "        Xs, Ys = self.get_arrays(split)\n",
    "\n",
    "        # --- índices opcionales ------------------------------------------------\n",
    "        if include_index:\n",
    "            if split == \"train\":\n",
    "                idx = self.train_idx\n",
    "            elif split == \"val\":\n",
    "                idx = self.val_idx\n",
    "            else:                               # split None  (o testset completo)\n",
    "                idx = np.arange(len(self.X), dtype=np.int64)\n",
    "\n",
    "            ds = tf.data.Dataset.from_tensor_slices((Xs, Ys, idx))\n",
    "        else:\n",
    "            ds = tf.data.Dataset.from_tensor_slices((Xs, Ys))\n",
    "\n",
    "        # --- barajado sólo en train -------------------------------------------\n",
    "        if shuffle and (split in (None, \"train\")):\n",
    "            ds = ds.shuffle(\n",
    "                buffer_size or len(Xs),\n",
    "                seed=seed,\n",
    "                reshuffle_each_iteration=True,\n",
    "            )\n",
    "\n",
    "        ds = ds.batch(batch_size)\n",
    "        if prefetch:\n",
    "            ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataModule:\n",
    "    \"\"\"\n",
    "    Descarga dos datasets de Kaggle y separa train / val (estratificado).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        local_download_dir: str,\n",
    "        # ------------------------------------------------------------------\n",
    "        keys: Optional[dict],\n",
    "        seed: int,\n",
    "    ):\n",
    "        self.seed = seed\n",
    "\n",
    "        # 2) TEST -----------------------------------------------------------\n",
    "        self.testset = SingleHDF5(\n",
    "            local_dir=local_download_dir,\n",
    "            keys={\"X\": \"X\", \"Y\": \"Y\", \"Z\": \"Z\"},\n",
    "        )\n",
    "\n",
    "    # ————————————————— API pública —————————————————\n",
    "    def get_arrays(self, split: Literal[\"train\", \"val\", \"test\"]):\n",
    "        if split in (\"train\", \"val\"):\n",
    "            return self.trainset.get_arrays(split)\n",
    "        if split == \"test\":\n",
    "            return self.testset.get_arrays()\n",
    "        raise ValueError(\"split debe ser 'train', 'val' o 'test'\")\n",
    "\n",
    "    def get_effects(self, **kw):\n",
    "        return self.testset.get_effects(**kw)\n",
    "    \n",
    "    def to_tf_dataset(\n",
    "        self,\n",
    "        *,\n",
    "        split: Literal[\"train\", \"val\", \"test\"],\n",
    "        batch_size: int,\n",
    "        shuffle: bool = True,\n",
    "        prefetch: bool = True,\n",
    "        **kw,\n",
    "    ):\n",
    "        common_kw = dict(\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            prefetch=prefetch,\n",
    "            seed=self.seed,\n",
    "            **kw,\n",
    "        )\n",
    "        return self.testset.to_tf_dataset(**common_kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo ARQ_2 ESC_1 rep_0... con el epoch_33.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 11:34:32.441669: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750437273.675691  146316 service.cc:145] XLA service 0x7f81efc0a1f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750437273.676462  146316 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "2025-06-20 11:34:33.678430: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-20 11:34:33.814643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1750437274.905693  146316 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔖 JSON de métricas y evaluación guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQ_2/ESC_1/_rep_0/reports/classification_report.json\n",
      "\n",
      "📄 Classification Report Summary\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bpsk     1.0000    1.0000    1.0000      1024\n",
      "        qpsk     0.9971    0.9990    0.9980      1024\n",
      "       16qam     0.7688    0.8604    0.8120      1024\n",
      " 32qam cross     0.6103    0.6729    0.6400      1024\n",
      "       64qam     0.8082    0.7285    0.7663      1024\n",
      "128qam cross     0.6484    0.5674    0.6052      1024\n",
      "\n",
      "    accuracy                         0.8047      6144\n",
      "   macro avg     0.8055    0.8047    0.8036      6144\n",
      "weighted avg     0.8055    0.8047    0.8036      6144\n",
      "\n",
      "\n",
      "Eval loss: 0.4513, Eval accuracy: 0.8047\n",
      "🔖 Gráfico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQ_2/ESC_1/_rep_0/reports/report_num_taps.png\n",
      "🔖 Gráfico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQ_2/ESC_1/_rep_0/reports/report_snr_db.png\n",
      "🔖 Gráfico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQ_2/ESC_1/_rep_0/reports/report_phase_offset.png\n",
      "🔖 Gráfico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQ_2/ESC_1/_rep_0/reports/report_roll_off.png\n",
      "🔖 Effects report JSON guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQ_2/ESC_1/_rep_0/reports/effects_report.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mclassification_report()\n\u001b[1;32m     64\u001b[0m analyzer\u001b[38;5;241m.\u001b[39meffect_report()\n\u001b[0;32m---> 65\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TESIS/AMC_ARQ/utils/analysis/analysis.py:129\u001b[0m, in \u001b[0;36mExperimentAnalyzer.confusion_matrix\u001b[0;34m(self, normalize, show_plots)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconfusion_matrix\u001b[39m(\u001b[38;5;28mself\u001b[39m, normalize: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, show_plots: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dibuja y guarda la matriz de confusión opcionalmente normalizada.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     labels \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names)))\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val, y_pred]))\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m     cm \u001b[38;5;241m=\u001b[39m confusion_matrix(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val, y_pred,\n\u001b[1;32m    137\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    138\u001b[0m         normalize\u001b[38;5;241m=\u001b[39mnormalize\n\u001b[1;32m    139\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/TESIS/AMC_ARQ/utils/analysis/analysis.py:387\u001b[0m, in \u001b[0;36mExperimentAnalyzer._predict_classes\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_predict_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predice clases y devuelve argmax sobre probabilidades softmax.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:566\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    564\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    565\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[0;32m--> 566\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[1;32m    568\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def parse_meta(p: Path):\n",
    "    parts = p.parts\n",
    "    arch = parts[-5]     # ARQ_*\n",
    "    esc  = parts[-4]     # ESC_*\n",
    "    rep  = int(parts[-3].split(\"_\")[1])\n",
    "    return arch, esc, rep\n",
    "\n",
    "def load_keras_model(p: Path):\n",
    "    \"\"\"\n",
    "    Carga un *modelo completo* (.keras).  Compila automáticamente con la\n",
    "    configuración original que quedó embebida en el checkpoint.\n",
    "    \"\"\"\n",
    "    return keras.models.load_model(p, compile=True) \n",
    "\n",
    "ARCHS = ['ARQ_2', 'ARQ_3']\n",
    "ESCS = [\"ESC_1\", \"ESC_2\", \"ESC_3\", \"ESC_4\", \"ESC_5\", \"ESC_6\", \"ESC_7\", \"ESC_8\", \"ESC_9\", \"ESC_10\", \"ESC_11\"]\n",
    "\n",
    "\n",
    "for arch_key in ARCHS:\n",
    "    for esc in ESCS:\n",
    "        test_data_dir = Path().resolve() / 'datasets' / 'test' / esc\n",
    "        dataset_path = sorted(test_data_dir.glob(\"*.hdf5\"))\n",
    "\n",
    "        for rep in range(10):\n",
    "            matches = [\n",
    "                p for p in keras_paths\n",
    "                if p.parts[-5] == arch_key and p.parts[-4] == esc\n",
    "                and int(p.parts[-3].split(\"_\")[1]) == rep\n",
    "            ]\n",
    "            if not matches:\n",
    "                continue\n",
    "\n",
    "            model_path = matches[0]          # p.e. epoch_12.keras\n",
    "            model = load_keras_model(model_path)\n",
    "            datamodule = DataModule(\n",
    "                local_download_dir=dataset_path[0],\n",
    "                keys={\"X\": \"X\", \"Y\": \"Y\", \"Z\": \"Z\"},\n",
    "                seed=42\n",
    "            )\n",
    "            cfg = load_config(f'{arch_key}_{esc}')\n",
    "            tr = cfg.get(\"training\", {})\n",
    "            test_ds_idx = datamodule.to_tf_dataset(\n",
    "                split=\"test\", batch_size=tr.get(\"batch_size\", 32),\n",
    "                shuffle=False, prefetch=False, include_index=True\n",
    "            )\n",
    "\n",
    "            # // Modificar subdirectorio de acuerdo a número actual de repetición  \\\\\n",
    "            cfg[\"experiment\"][\"output_subdir\"] = cfg[\"experiment\"][\"output_subdir\"] + \"/\" + f\"_rep_{rep}\"\n",
    "\n",
    "            print(f\"Evaluando modelo {arch_key} {esc} rep_{rep}... con el {model_path.name}\")\n",
    "            #  4A.6) Análisis resultados individual\n",
    "            from utils.analysis.analysis import ExperimentAnalyzer\n",
    "            analyzer = ExperimentAnalyzer(\n",
    "                model=model,\n",
    "                history=None,\n",
    "                test_data=test_ds_idx,\n",
    "                cfg=cfg,\n",
    "                effects=datamodule.get_effects(),\n",
    "                repeat_index=rep,\n",
    "                show_plots=False,\n",
    "                )\n",
    "\n",
    "            analyzer.classification_report()\n",
    "            analyzer.effect_report()\n",
    "            analyzer.confusion_matrix(normalize=\"true\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
