{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1748971783603,
     "user": {
      "displayName": "Juan Pablo Perez Vargas",
      "userId": "11361655634266533980"
     },
     "user_tz": 300
    },
    "id": "0XyIyd0J2Pa9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juanpabloperezvargas/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-25 22:13:10.361892: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/juanpabloperezvargas/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Colab: asegurar versiones recientes\n",
    "# !pip install -q scikit-posthocs==0.7 seaborn==0.13\n",
    "# !pip install --upgrade \"tensorflow==2.16.*\" \"keras==3.*\"\n",
    "# !pip install scikit-learn\n",
    "# !pip install kaggle\n",
    "# !pip install --upgrade keras==2.10.0\n",
    "\n",
    "from pathlib import Path\n",
    "import json, math, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "\n",
    "STAGE = \"ARQs\" \n",
    "# STAGE = \"256_QAM\" \n",
    "\n",
    "os.environ[\"workplace\"] = \"local\" # \"colab\" or \"local\"\n",
    "\n",
    "if os.environ[\"workplace\"] == \"colab\":\n",
    "    BASE_DIR     = Path('/content/drive/MyDrive/structure')\n",
    "    # 1) Montar Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "    # 2) Rutas base\n",
    "    MODELS_DIR   = BASE_DIR / 'models'\n",
    "\n",
    "    # 3) Path y sys.path\n",
    "    import sys\n",
    "    sys.path.append(str(MODELS_DIR))\n",
    "    \n",
    "else:\n",
    "    BASE_DIR     = Path().resolve()\n",
    "\n",
    "\n",
    "CONFIG_ROOT = BASE_DIR / \"configs\"\n",
    "MODELS_ROOT = BASE_DIR / \"models\"\n",
    "RESULTS_ROOT = BASE_DIR / \"results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 330 modelos Keras\n"
     ]
    }
   ],
   "source": [
    "def discover_keras(root: Path):\n",
    "    \"\"\"\n",
    "    Devuelve rutas a todos los modelos Keras encontrados en el directorio\n",
    "    especificado. Se espera que los modelos est√©n en subdirectorios con\n",
    "    ARQ_*/ESC_*/rep_*/checkpoints/.\n",
    "    \"\"\"\n",
    "    return sorted(root.glob(\"ARQ_*/ESC_*/rep_*/checkpoints/*.keras\"))\n",
    "\n",
    "TARGET_PATH = RESULTS_ROOT / STAGE\n",
    "keras_paths = discover_keras(TARGET_PATH)\n",
    "print(f\"Encontrados {len(keras_paths)} modelos Keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module.py\n",
    "import numpy as np\n",
    "from typing import Literal, Optional\n",
    "import os, h5py, numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "import yaml                          # Lectura y mezcla de archivos YAML\n",
    "import sys               # Conversi√≥n IPython Notebook ‚Üí .py\n",
    "from pathlib import Path             # Manejo robusto de rutas\n",
    "from importlib import import_module  # Import din√°mico del modelo\n",
    "\n",
    "def load_config(stage:str, exp_name:str):\n",
    "    exp_path = CONFIG_ROOT / \"experiments\" / f\"{stage}\" / f\"{exp_name}.yaml\"\n",
    "    exp_cfg  = yaml.safe_load(exp_path.read_text())\n",
    "\n",
    "    if \"_base_\" in exp_cfg:                                # herencia opcional\n",
    "        base_cfg = yaml.safe_load((CONFIG_ROOT / exp_cfg[\"_base_\"]).read_text())\n",
    "        cfg = {**base_cfg, **exp_cfg}                      # exp > default\n",
    "    else:\n",
    "        cfg = exp_cfg\n",
    "    return cfg\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Configuraci√≥n de Kaggle API\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"ilikepizzaanddrones\"\n",
    "os.environ[\"KAGGLE_KEY\"]      = \"b7d0370fced8eb934d226172fff8221f\"\n",
    "\n",
    "try:\n",
    "    from kaggle import KaggleApi\n",
    "except ModuleNotFoundError:\n",
    "    raise ImportError(\"pip install kaggle\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class SingleHDF5:\n",
    "    \"\"\"\n",
    "    Envuelve *un* .hdf5 proveniente de Kaggle.\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    kaggle_dataset_id : str\n",
    "        slug ¬´user/dataset¬ª (ej. \"ilikepizzaanddrones/modulated-iq-signals\")\n",
    "    local_download_dir : str | Path\n",
    "        carpeta donde se guardar√° (y se buscar√°) el .hdf5\n",
    "    keys : dict | None\n",
    "        nombres de los grupos dentro del HDF5 (default {\"X\",\"Y\",\"Z\"})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        kaggle_dataset_id: str,\n",
    "        local_download_dir: str,\n",
    "        keys: dict,\n",
    "    ) -> None:\n",
    "\n",
    "        # 0) Descarga / b√∫squeda local\n",
    "        file_path = self._download_if_needed(kaggle_dataset_id, local_download_dir)\n",
    "\n",
    "        # 1) Lectura a memoria\n",
    "        self.keys = keys or {\"X\": \"X\", \"Y\": \"Y\", \"Z\": \"Z\"}\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            self.X = f[self.keys[\"X\"]][:]\n",
    "            self.Y = f[self.keys[\"Y\"]][:]\n",
    "            self.Z = f[self.keys[\"Z\"]][:] if self.keys[\"Z\"] in f else None\n",
    "\n",
    "            if \"Effects\" in f:\n",
    "                grp   = f[\"Effects\"]\n",
    "                dtype = [(n, grp[n].dtype) for n in grp]\n",
    "                eff   = np.empty(len(self.X), dtype=dtype)\n",
    "                for n in grp: eff[n] = grp[n][:]\n",
    "                self.Effects = eff\n",
    "            else:\n",
    "                self.Effects = None\n",
    "\n",
    "        # √≠ndices activos (se sobre-escriben desde DataModule)\n",
    "        n = len(self.X)\n",
    "        self.train_idx = np.arange(n, dtype=np.int64)\n",
    "        self.val_idx   = np.empty(0, dtype=np.int64)\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    @staticmethod\n",
    "    def _download_if_needed(kaggle_dataset_id, local_dir):\n",
    "        local_dir = Path(local_dir)\n",
    "        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        h5_files = sorted(local_dir.rglob(\"*.hdf5\"))\n",
    "        if not h5_files:          # primera vez\n",
    "            print(f\"‚¨áÔ∏è  Descargando ¬´{kaggle_dataset_id}¬ª ‚Ä¶\")\n",
    "            api = KaggleApi(); api.authenticate()\n",
    "            api.dataset_download_files(\n",
    "                kaggle_dataset_id,\n",
    "                path=str(local_dir),\n",
    "                unzip=True,\n",
    "                quiet=False,\n",
    "            )\n",
    "            h5_files = sorted(local_dir.rglob(\"*.hdf5\"))\n",
    "\n",
    "        if not h5_files:\n",
    "            raise FileNotFoundError(\"No se encontr√≥ ning√∫n .hdf5 en el zip\")\n",
    "        if len(h5_files) > 1:\n",
    "            raise ValueError(\"Hay >1 .hdf5 descargado; limpia la carpeta o elige uno.\")\n",
    "        return h5_files[0]\n",
    "\n",
    "    # ‚îÄ‚îÄ API m√≠nima (igual que antes) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def register_indices(self, train_idx, val_idx):\n",
    "        self.train_idx = np.asarray(train_idx, dtype=np.int64)\n",
    "        self.val_idx   = np.asarray(val_idx,   dtype=np.int64)\n",
    "\n",
    "    def get_arrays(self, split: str = None):\n",
    "        if split is None: return self.X, self.Y\n",
    "        split = split.lower()\n",
    "        if split == \"train\": return self.X[self.train_idx], self.Y[self.train_idx]\n",
    "        if split == \"val\":   return self.X[self.val_idx],   self.Y[self.val_idx]\n",
    "        raise ValueError(\"split debe ser 'train' o 'val'\")\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "    def get_effects(\n",
    "        self,\n",
    "        *,\n",
    "        split: str = None,\n",
    "        fields: list = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Devuelve un structured-array con los efectos alineados al `split`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        split : \"train\" | \"val\" | None\n",
    "            None ‚áí dataset completo (o testset completo si proviene del DataModule).\n",
    "        fields : list[str] | None\n",
    "            Sub-conjunto de columnas a devolver. None ‚áí todas.\n",
    "        \"\"\"\n",
    "        if self.Effects is None:\n",
    "            raise ValueError(\"Este HDF5 no contiene grupo 'Effects'.\")\n",
    "\n",
    "        # Selecci√≥n de √≠ndices seg√∫n split\n",
    "        if split is None:\n",
    "            idx = (\n",
    "                np.arange(len(self.X))               # testset completo\n",
    "                if (not hasattr(self, \"train_idx\"))   # por seguridad\n",
    "                else self.train_idx                   # SingleHDF5 sin register\n",
    "            )\n",
    "        else:\n",
    "            split = split.lower()\n",
    "            if split == \"train\":\n",
    "                idx = self.train_idx\n",
    "            elif split == \"val\":\n",
    "                idx = self.val_idx\n",
    "            else:\n",
    "                raise ValueError(\"split debe ser 'train', 'val' o None\")\n",
    "\n",
    "        eff = self.Effects[idx]              # vista alineada\n",
    "        if fields is not None:\n",
    "            eff = eff[fields].copy()\n",
    "        return eff\n",
    "    \n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    \n",
    "    def to_tf_dataset(\n",
    "        self,\n",
    "        *,                                      \n",
    "        split: str = None,\n",
    "        batch_size: int,\n",
    "        shuffle: bool = True,\n",
    "        seed: int,\n",
    "        prefetch: bool = True,\n",
    "        include_index: bool = False,\n",
    "        buffer_size: int = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Devuelve un tf.data.Dataset con (X, Y) o (X, Y, idx).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        split : \"train\" | \"val\" | None\n",
    "            None ‚áí dataset completo (sin barajar).\n",
    "        include_index : bool\n",
    "            Si True, a√±ade el √≠ndice absoluto dentro del HDF5\n",
    "            (√∫til para m√©tricas por muestra).\n",
    "        buffer_size : int | None\n",
    "            Tama√±o del ¬´shuffle buffer¬ª. Por defecto = len(split).\n",
    "        \"\"\"\n",
    "\n",
    "        Xs, Ys = self.get_arrays(split)\n",
    "\n",
    "        # --- √≠ndices opcionales ------------------------------------------------\n",
    "        if include_index:\n",
    "            if split == \"train\":\n",
    "                idx = self.train_idx\n",
    "            elif split == \"val\":\n",
    "                idx = self.val_idx\n",
    "            else:                               # split None  (o testset completo)\n",
    "                idx = np.arange(len(self.X), dtype=np.int64)\n",
    "\n",
    "            ds = tf.data.Dataset.from_tensor_slices((Xs, Ys, idx))\n",
    "        else:\n",
    "            ds = tf.data.Dataset.from_tensor_slices((Xs, Ys))\n",
    "\n",
    "        # --- barajado s√≥lo en train -------------------------------------------\n",
    "        if shuffle and (split in (None, \"train\")):\n",
    "            ds = ds.shuffle(\n",
    "                buffer_size or len(Xs),\n",
    "                seed=seed,\n",
    "                reshuffle_each_iteration=True,\n",
    "            )\n",
    "\n",
    "        ds = ds.batch(batch_size)\n",
    "        if prefetch:\n",
    "            ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "\n",
    "class DataModule:\n",
    "    \"\"\"\n",
    "    Descarga dos datasets de Kaggle y separa train / val (estratificado).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        # --- TEST ----------------------------------------------------------\n",
    "        test_kaggle_dataset_id: str,\n",
    "        test_local_download_dir: str,\n",
    "        keys: Optional[dict],\n",
    "        seed: int,\n",
    "    ):\n",
    "        self.seed = seed\n",
    "\n",
    "        # 2) TEST -----------------------------------------------------------\n",
    "        self.testset = SingleHDF5(\n",
    "            kaggle_dataset_id=test_kaggle_dataset_id,\n",
    "            local_download_dir=test_local_download_dir,\n",
    "            keys=keys or {\"X\": \"X\", \"Y\": \"Y\", \"Z\": \"Z\"},\n",
    "        )\n",
    "\n",
    "    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî API p√∫blica ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "    def get_arrays(self, split: Literal[\"train\", \"val\", \"test\"]):\n",
    "        if split in (\"train\", \"val\"):\n",
    "            return self.trainset.get_arrays(split)\n",
    "        if split == \"test\":\n",
    "            return self.testset.get_arrays()\n",
    "        raise ValueError(\"split debe ser 'train', 'val' o 'test'\")\n",
    "\n",
    "    def get_effects(self, **kw):\n",
    "        return self.testset.get_effects(**kw)\n",
    "    \n",
    "    def to_tf_dataset(\n",
    "        self,\n",
    "        *,\n",
    "        split: Literal[\"train\", \"val\", \"test\"],\n",
    "        batch_size: int,\n",
    "        shuffle: bool = True,\n",
    "        prefetch: bool = True,\n",
    "        **kw,\n",
    "    ):\n",
    "        common_kw = dict(\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            prefetch=prefetch,\n",
    "            seed=self.seed,\n",
    "            **kw,\n",
    "        )\n",
    "        return self.testset.to_tf_dataset(**common_kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando ARQ_2 en ESC_1 rep_0 con el modelo epoch_14.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 22:13:20.333432: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750907601.652524 1443686 service.cc:145] XLA service 0x7faa22faecc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1750907601.652570 1443686 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "2025-06-25 22:13:21.652928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-25 22:13:21.696604: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1750907602.493203 1443686 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîñ JSON de m√©tricas y evaluaci√≥n guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/classification_report.json\n",
      "\n",
      "üìÑ Classification Report Summary\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bpsk     1.0000    0.9990    0.9995      1024\n",
      "        qpsk     1.0000    1.0000    1.0000      1024\n",
      "       16qam     0.3121    0.1484    0.2012      1024\n",
      " 32qam cross     0.4244    0.5703    0.4867      1024\n",
      "       64qam     0.4983    0.7021    0.5829      1024\n",
      "128qam cross     0.3072    0.2373    0.2678      1024\n",
      "\n",
      "    accuracy                         0.6095      6144\n",
      "   macro avg     0.5903    0.6095    0.5897      6144\n",
      "weighted avg     0.5903    0.6095    0.5897      6144\n",
      "\n",
      "\n",
      "Eval loss: 0.8280, Eval accuracy: 0.6095\n",
      "üîñ Gr√°fico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/report_num_taps.png\n",
      "üîñ Gr√°fico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/report_snr_db.png\n",
      "üîñ Gr√°fico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/report_phase_offset.png\n",
      "üîñ Gr√°fico guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/report_roll_off.png\n",
      "üîñ Effects report JSON guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/effects_report.json\n",
      "üîñ Imagen guardada en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/confusion_matrix.png\n",
      "üîñ JSON guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/confusion_matrix.json\n",
      "üîñ CSV de predicciones guardado en: /Users/juanpabloperezvargas/Desktop/TESIS/AMC_ARQ/outputs/ARQs/ARQ_2/ESC_1/rep_0/reports/predictions.csv\n",
      "Evaluando ARQ_2 en ESC_1 rep_1 con el modelo epoch_17.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 22:20:42.542461: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExperimentAnalyzer\n\u001b[1;32m     56\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m ExperimentAnalyzer(\n\u001b[1;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     58\u001b[0m     history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     show_plots\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m     )\n\u001b[0;32m---> 66\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m analyzer\u001b[38;5;241m.\u001b[39meffect_report()\n\u001b[1;32m     68\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mconfusion_matrix(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/TESIS/AMC_ARQ/utils/analysis/analysis.py:229\u001b[0m, in \u001b[0;36mExperimentAnalyzer.classification_report\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m val_acc  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(results[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# 3) Generar el dict completo del classification_report\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m report_dict \u001b[38;5;241m=\u001b[39m sk_cr(\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_val,           \u001b[38;5;66;03m# etiquetas en √≠ndice\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# 4) Construir objeto JSON con metadata y resultados\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TESIS/AMC_ARQ/utils/analysis/analysis.py:496\u001b[0m, in \u001b[0;36mExperimentAnalyzer._predict_classes\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_predict_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    495\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predice clases y devuelve argmax sobre probabilidades softmax.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:566\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    564\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    565\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[0;32m--> 566\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[1;32m    568\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def parse_meta(p: Path):\n",
    "    parts = p.parts\n",
    "    arch = parts[-5]     # ARQ_*\n",
    "    esc  = parts[-4]     # ESC_*\n",
    "    rep  = int(parts[-3].split(\"_\")[1])\n",
    "    return arch, esc, rep\n",
    "\n",
    "def load_keras_model(p: Path):\n",
    "    \"\"\"\n",
    "    Carga un *modelo completo* (.keras).  Compila autom√°ticamente con la\n",
    "    configuraci√≥n original que qued√≥ embebida en el checkpoint.\n",
    "    \"\"\"\n",
    "    return keras.models.load_model(p, compile=True) \n",
    "\n",
    "ARCHS = ['ARQ_2', 'ARQ_2', 'ARQ_3']\n",
    "ESCS = [\"ESC_1\", \"ESC_2\", \"ESC_3\", \"ESC_4\", \"ESC_5\", \"ESC_6\", \"ESC_7\", \"ESC_8\", \"ESC_9\", \"ESC_10\", \"ESC_11\"]\n",
    "\n",
    "\n",
    "for arch_key in ARCHS:\n",
    "    for esc in ESCS:\n",
    "        test_data_dir = Path().resolve() / 'datasets' / 'test' / esc\n",
    "        dataset_path = sorted(test_data_dir.glob(\"*.hdf5\"))\n",
    "        for rep in range(10):\n",
    "            matches = [\n",
    "                p for p in keras_paths\n",
    "                if p.parts[-5] == arch_key and p.parts[-4] == esc\n",
    "                and int(p.parts[-3].split(\"_\")[1]) == rep\n",
    "            ]\n",
    "            if not matches:\n",
    "                continue\n",
    "\n",
    "            cfg = load_config(stage=STAGE, exp_name=f'{arch_key}_{esc}')\n",
    "\n",
    "            model_path = matches[0]          # p.e. epoch_12.keras\n",
    "\n",
    "            model = load_keras_model(model_path)\n",
    "            datamodule = DataModule(\n",
    "                test_kaggle_dataset_id=cfg[\"dataset\"][\"kaggle\"][\"test\"][\"dataset_id\"],\n",
    "                test_local_download_dir=cfg[\"dataset\"][\"kaggle\"][\"test\"][\"download_dir\"],\n",
    "                keys={\"X\": \"X\", \"Y\": \"Y\", \"Z\": \"Z\"},\n",
    "                seed=cfg[\"training\"][\"seed\"],\n",
    "            )\n",
    "            tr = cfg.get(\"training\", {})\n",
    "            test_ds_idx = datamodule.to_tf_dataset(\n",
    "                split=\"test\", batch_size=tr.get(\"batch_size\", 32),\n",
    "                shuffle=False, prefetch=False, include_index=True\n",
    "            )\n",
    "\n",
    "            # // Modificar subdirectorio de acuerdo a n√∫mero actual de repetici√≥n  \\\\\n",
    "            cfg[\"experiment\"][\"output_root\"] = cfg[\"experiment\"][\"output_root\"].replace(\"outputs/\", \"outputs/\" + STAGE + \"/\")\n",
    "            cfg[\"experiment\"][\"output_subdir\"] = cfg[\"experiment\"][\"output_subdir\"] + \"/\" + f\"rep_{rep}\"\n",
    "\n",
    "            print(f\"Evaluando {arch_key} en {esc} rep_{rep} con el modelo {model_path.name}\")\n",
    "            #  4A.6) An√°lisis resultados individual\n",
    "            from utils.analysis.analysis import ExperimentAnalyzer\n",
    "            analyzer = ExperimentAnalyzer(\n",
    "                model=model,\n",
    "                history=None,\n",
    "                test_data=test_ds_idx,\n",
    "                cfg=cfg,\n",
    "                effects=datamodule.get_effects(),\n",
    "                repeat_index=rep,\n",
    "                show_plots=False,\n",
    "                )\n",
    "\n",
    "            analyzer.classification_report()\n",
    "            analyzer.effect_report()\n",
    "            analyzer.confusion_matrix(normalize=\"true\")\n",
    "            analyzer.predictions_csv(save_probs=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
